{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFPFbI-nhni_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten\n",
        "\n",
        "# Set parameters\n",
        "max_words = 10000 # Consider only the top 10,000 most frequently occurring words\n",
        "max_length = 250 # Limit the review length to 250 words\n",
        "embedding_size = 50 # Dimensionality of the word embeddings\n",
        "# Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Preprocess the data\n",
        "#following function transforms a list of sequences into a 2D Numpy array\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "#The Embedding layer is defined as the first hidden layer of a network.\n",
        "model.add(Embedding(max_words, embedding_size, input_length=max_length))\n",
        "#If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.05)\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ]
    }
  ]
}